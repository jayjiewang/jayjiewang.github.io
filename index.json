[{"authors":["libuyu"],"categories":null,"content":"Buyu Li is a PhD candidate at Multimedia Lab (MMLab), The Chinese University of Hong Kong, supervised by Prof. Xiaogang Wang. He also has a close research collaboration with Yu Liu, Quanquan Li, Junjie Yan and Prof. Wanli Ouyang.\nHe used to work as a computer vision researcher in Sensetime Research (2016-2017). During this period, he was a member of the CUImage team that won the first place in ILSVRC2016 DET (ImageNet).\nHis research interests now include but not limit to object detection (both 2D and 3D), and 3D animation.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"e6e14d1b2af2d0d304f4e23dad6a560a","permalink":"https://libuyu.github.io/authors/libuyu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/libuyu/","section":"authors","summary":"Buyu Li is a PhD candidate at Multimedia Lab (MMLab), The Chinese University of Hong Kong, supervised by Prof. Xiaogang Wang. He also has a close research collaboration with Yu Liu, Quanquan Li, Junjie Yan and Prof.","tags":null,"title":"Buyu Li","type":"authors"},{"authors":["Jingru Tan","Changbao Wang","**Buyu Li**","Quanquan Li","Wanli Ouyang","Changqing Yin","Junjie Yan"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"75e5358d7472e9086d9a99988903bc77","permalink":"https://libuyu.github.io/publication/eql/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/eql/","section":"publication","summary":"Object recognition techniques using convolutional neural networks (CNN) have achieved great success. However, state-of-the-art object detection methods still perform poorly on large vocabulary and long-tailed datasets, e.g. LVIS. In this work, we analyze this problem from a novel perspective: each positive sample of one category can be seen as a negative sample for other categories, making the tail categories receive more discouraging gradients. Based on it, we propose a simple but effective loss, named equalization loss, to tackle the problem of long-tailed rare categories by simply ignoring those gradients for rare categories. The equalization loss protects the learning of rare categories from being at a disadvantage during the network parameter updating. Thus the model is capable of learning better discriminative features for objects of rare classes. Without any bells and whistles, our method achieves AP gains of 4.1% and 4.8% for the rare and common categories on the challenging LVIS benchmark, compared to the Mask R-CNN baseline. With the utilization of the effective equalization loss, we finally won the 1st place in the LVIS Challenge 2019.","tags":null,"title":"Equalization Loss for Long-Tailed Object Recognition (CVPR 2020)","type":"publication"},{"authors":["Yingjie Cai","**Buyu Li**","Zeyu Jiao","Hongsheng Li","Xingyu Zeng","Xiaogang Wang"],"categories":null,"content":"","date":1577808000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577808000,"objectID":"6cbecea18955e732078533bd7f0bd5e5","permalink":"https://libuyu.github.io/publication/decouple3d/","publishdate":"2020-01-01T00:00:00+08:00","relpermalink":"/publication/decouple3d/","section":"publication","summary":"Monocular 3D object detection task aims to predict the 3D bounding boxes of objects based on monocular RGB images. Since the location recovery in 3D space is quite difficult on account of absence of depth information, this paper proposes a novel unified framework which decomposes the detection problem into a structured polygon prediction task and a depth recovery task. Different from the widely studied 2D bounding boxes, the proposed novel structured polygon in the 2D image consists of several projected surfaces of the target object. Compared to the widely-used 3D bounding box proposals, it is shown to be a better representation for 3D detection. In order to inversely project the predicted 2D structured polygon to a cuboid in the 3D physical world, the following depth recovery task uses the object height prior to complete the inverse projection transformation with the given camera projection matrix. Moreover, a fine-grained 3D box refinement scheme is proposed to further rectify the 3D detection results. Experiments are conducted on the challenging KITTI benchmark, in which our method achieves state-of-the-art detection accuracy.","tags":[],"title":"Monocular 3D Object Detection with Decoupled Structured Polygon Estimation and Height-Guided Depth Estimation (AAAI 2020)","type":"publication"},{"authors":["**Buyu Li**","Wanli Ouyang","Lu Sheng","Xingyu Zeng","Xiaogang Wang"],"categories":null,"content":"","date":1560614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560614400,"objectID":"204261f041f21edbbc0b3fb069eac8da","permalink":"https://libuyu.github.io/publication/gs3d/","publishdate":"2019-06-16T00:00:00+08:00","relpermalink":"/publication/gs3d/","section":"publication","summary":"We present an efficient 3D object detection framework based on a single RGB image in the scenario of autonomous driving. Our efforts are put on extracting the underlying 3D information in a 2D image and determining the accurate 3D bounding box of the object without point cloud or stereo data. Leveraging the off-the-shelf 2D object detector, we propose an artful approach to efficiently obtain a coarse cuboid for each predicted 2D box. The coarse cuboid has enough accuracy to guide us to determine the 3D box of the object by refinement. In contrast to previous state-of-the-art methods that only use the features extracted from the 2D bounding box for box refinement, we explore the 3D structure information of the object by employing the visual features of visible surfaces. The new features from surfaces are utilized to eliminate the problem of representation ambiguity brought by only using a 2D bounding box. Moreover, we investigate different methods of 3D box refinement and discover that a classification formulation with quality aware loss has much better performance than regression. Evaluated on the KITTI benchmark, our approach outperforms current state-of-the-art methods for single RGB image based 3D object detection.","tags":[],"title":"GS3D: An Efficient 3D Object Detection Framework for Autonomous Driving (CVPR 2019)","type":"publication"},{"authors":["**Buyu Li**","Yu Liu","Xiaogang Wang"],"categories":null,"content":"","date":1548518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548518400,"objectID":"3a6eff49f4467d4b56a38f859b071dac","permalink":"https://libuyu.github.io/publication/ghm/","publishdate":"2019-01-27T00:00:00+08:00","relpermalink":"/publication/ghm/","section":"publication","summary":"Despite the great success of two-stage detectors, single-stage detector is still a more elegant and efficient way, yet suffers from the two well-known disharmonies during training, i.e. the huge difference in quantity between positive and negative examples as well as between easy and hard examples. In this work, we first point out that the essential effect of the two disharmonies can be summarized in term of the gradient. Further, we propose a novel gradient harmonizing mechanism (GHM) to be a hedging for the disharmonies. The philosophy behind GHM can be easily embedded into both classification loss function like cross-entropy (CE) and regression loss function like smooth-$L_1$ ($SL_1$) loss. To this end, two novel loss functions called GHM-C and GHM-R are designed to balancing the gradient flow for anchor classification and bounding box refinement, respectively. Ablation study on MS COCO demonstrates that without laborious hyper-parameter tuning, both GHM-C and GHM-R can bring substantial improvement for single stage detector. Without any whistles and bells, our model achieves 41.6 mAP on COCO test-dev set which surpass the state-of-the-art method, Focal Loss+$SL_1$, by 0.8.","tags":[],"title":"Gradient Harmonized Single-stage Detector (AAAI 2019 Oral)","type":"publication"},{"authors":["Xin Lu","**Buyu Li**","Yuxin Yue","Quanquan Li","Junjie Yan"],"categories":null,"content":"","date":1548518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548518400,"objectID":"0f8153d8d1361da591cfab9c77e59b47","permalink":"https://libuyu.github.io/publication/gridrcnn/","publishdate":"2019-01-27T00:00:00+08:00","relpermalink":"/publication/gridrcnn/","section":"publication","summary":"This paper proposes a novel object detection framework named Grid R-CNN, which adopts a grid guided localization mechanism for accurate object detection. Different from the traditional regression based methods, the Grid R-CNN captures the spatial information explicitly and enjoys the position sensitive property of fully convolutional architecture. Instead of using only two independent points, we design a multi-point supervision formulation to encode more clues in order to reduce the impact of inaccurate prediction of specific points. To take the full advantage of the correlation of points in a grid, we propose a two-stage information fusion strategy to fuse feature maps of neighbor grid points. The grid guided localization approach is easy to be extended to different state-of-the-art detection frameworks. Grid R-CNN leads to high quality object localization, and experiments demonstrate that it achieves a 4.1% AP gain at IoU=0.8 and a 10.0% AP gain at IoU=0.9 on COCO benchmark compared to Faster R-CNN with Res50 backbone and FPN architecture.","tags":[],"title":"Grid R-CNN (CVPR 2019)","type":"publication"}]